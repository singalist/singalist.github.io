---
layout: default
tags: about
---
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script type="text/javascript">
  function readMore() {
    $('#readMore').hide();
    $('#more').show();
  }
  function readLess() {
    $('#readMore').show();
    $('#more').hide();
  }
</script>
<img src="{{ site.baseurl }}/images/me.jpg" alt="Binhui Xie" width="180"
  style="float: right; padding: 5px 0 0 15px; border-radius: 0%;" />

<div class="bio" style="text-align:justify">
  <!-- <hr/> -->
  <p>
    I am a fourth-year Ph.D. student at Beijing Institute of Technology, advised by <a href="https://shuangli.xyz"
      class="uline">Prof. Shuang Li</a> and Prof. Chi Harold Liu. I received my Bachelor's degree in Software
    Engineering from <a href="https://www.bit.edu.cn" class="uline">BIT</a> (2015-2019).
  </p>

  <p>
    My research interests are in foundation models, transfer learning, data-efficient learning (particularly domain adaptation, active learning) and self-supervised learning.
  </p>

  <p>
    <b>Contact</b>: I'm always happy to discuss or collaborate! Feel free to drop me an <a
      href="mailto:binhuixie@bit.edu.cn" class="uline">email</a> if you're interested.
  </p>

  <br />

  <!-- <div class="container">
    <div class="row" style="text-align:center;">
      <div class="col-4">
        <div class="row">
          <div class="col-6">
            <a href="http://www.bits-pilani.ac.in/"><img src="images/bits.png" style="max-height:150px;width:80%"></a>
          </div>
          <div class="col-6">
            <a href="https://www.gatech.edu/"><img src="images/gt.jpg" style="max-height:150px;width:80%"></a>
          </div>
        </div>
      </div>
      <div class="col-8">
        <div class="row" style="text-align:center;">
          <div class="col-3">
            <a href="https://www.adobe.com/in/"><img src="images/adobe.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="http://curai.com/"><img src="images/curai.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="https://einstein.ai/"><img src="images/sf.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="https://nv-tlabs.github.io/"><img src="images/nvidia.png" style="width:80%;max-height:150px"></a>
          </div>
        </div>
      </div>
    </div>
    <div class="row" style="text-align:center;">
      <div class="col-4">
        <div class="row">
          <div class="col-6">
            <div style="padding:10px">
              <h6>Fall '11-Spring '15</h6>
            </div>
          </div>
          <div class="col-6">
            <div style="padding:10px">
              <h6>Fall '17-current</h6>
            </div>
          </div>
        </div>
      </div>
      <div class="col-8">
        <div class="row" style="text-align:center;">
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '14, Fall '15- Summer '16</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '18, '19</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '21</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '22</h6>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div> -->
  
</div>


<hr />
<div class="news">
  <h2>News</h2>
  <br />
  <ul>

    <li>
      <p>[Feb 2023] Two papers on function model (<a href="https://arxiv.org/abs/2211.07636" class="uline"> 
        EVA</a>) and test-time adaptation (<a href="binhuixie.github.io" class="uline"> RoTTA</a>) are accepted by CVPR 2023!</p>
    </li>
    <li>
        <p>[Jan 2023] Our <a href="https://arxiv.org/abs/2204.08808" class="uline">SePiCo</a> for domain adaptive semantic segmentation is accepted by <font color="red">T-PAMI</font> (IF: 24.31)!</p>
      </li>

    <li>
      <p>[Nov 2022] Our <a href="https://arxiv.org/abs/2211.12256" class="uline"> VBLC </a> for semantic
        segmentation under adverse conditions is accepted by AAAI 2023 (<font color="red">ORAL</font>)!</p>
    </li>

    <!-- <li>
      <p>[Nov 2022] <a href='https://arxiv.org/abs/2211.07636' class='uline'>EVA</a> Unit-01 Launched!</p>
    </li> -->

    <li>
      <p>[Jun 2022] Our <a href="https://ieeexplore.ieee.org/document/9803869" class="uline">CAF</a> for domain adaptaion is accepted by TKDE!</p>
    </li>

    <li>
      <p>[Apr 2022] Our <a href='https://ieeexplore.ieee.org/document/9756672' class='uline'>CSDN</a> for
        partial domain adaptation is accepted by T-CYB!</p>
    </li>

    <li>
      <p>[Mar 2022] Our <a href="https://arxiv.org/abs/2111.12940" class="uline"> RIPU</a> for data-efficient semantic segmentation is accepted by CVPR 2022  (<font color="red">ORAL</font>)!</p>
    </li>

    <li>
      <p>[Dec 2021] Our <a href='https://arxiv.org/abs/2112.01406' class='uline'>EADA</a> for active domain adaptation is accepted by AAAI 2022!</p>
    </li>

    <li>
      <p>[Feb 2021] Our <a href='https://arxiv.org/abs/2103.12339' class='uline'>GDCAN</a> (Journal Version of <a href='https://arxiv.org/abs/2005.06717' class='uline'>DCAN</a>) is accepted by <font color="red">T-PAMI</font> (IF: 24.31)!</p>
    </li>


    <li>
      <p>[Dec 2020] Our <a href='https://arxiv.org/abs/2012.06995' class='uline'>BCDM</a> for
        bi-classifier adversarial adaptation is accepted by AAAI 2021!</p>
    </li>

    <li>
      <p>[Jul 2020] Our <a href='https://arxiv.org/abs/2008.01677' class='uline'>SSAN</a> for
        heterogeneous domain adaptation is accepted by ACM MM 2020!</p>
    </li>

  </ul>
  <a id="readMore" class="uline" href="#" onclick="readMore();return false;">
    <div style='text-align: center;'>
      <h4>Read More</h4>
    </div>
  </a>
  <span id="more">
    <ul>

      <li>
        <p> [Jan 2020] Our <a href="https://ieeexplore.ieee.org/document/8951259" class="uline">DTLC</a> for shallow domain adpatation is accepted by TNNLS! </p>
    </li>

      <li>
        <p>[Dec 2019] Our <a href='https://arxiv.org/abs/2005.06717' class='uline'>DCAN</a> for domain
          adaptation is accepted by AAAI 2020!</p>
      </li>

      <li>
        <p>[Jul 2019] Our <a href='https://dl.acm.org/doi/abs/10.1145/3343031.3351070' class='uline'>JADA</a> (my starting research journey) is accepted by ACM MM 2019!</p>
      </li>
  </span>
  </ul>
  <a id="readLess" class="uline" href="#" onclick="readLess();return false;">
    <div style='text-align: center;'>
      <h4>Read Less</h4>
    </div>
  </a>
</div>

<hr />

<!-- <hr>
<div class="Achievements">
  <h1>Achievements</h1>
  <ul>
    <li>
      <p>Among the top 30 percent highest scoring reviewers for NeurIPS 2018.</p>
    </li>
    <li>
      <p>Winner of <a href="https://vthacks-iv.devpost.com/submissions" class="uline">VT Hacks 2017</a>, a <a href="https://mlh.io/" class="uline">Major League Hacking</a> event.</p>
    </li>
    <li>
      <p>Awarded travel scholarship to represent CloudCV at Google Summer of Code Mentor Summit 2016, 2017.</p>
    </li>
    <li>
      <p>Winner, Google Hackathon, <a href="https://bits-apogee.org/" class="uline">APOGEE</a> 2014 (BITS Pilani's annual technical symposium).</p>
    </li>
    <li>
      <p>Second Place, Project Presentation (Adaptive Technology and Design Appliances Tracks), <a href="https://bits-apogee.org/" class="uline">APOGEE</a> 2013.</p>
    </li>
    <li>
    <p>Top 0.2%, <a href="http://bitsadmission.com/bitsatmain.aspx" class="uline">BITSAT</a> 2011 (from >120k students).</p>
    </li>
    <li>
      <p>Top 0.1%, <a href="http://www.cisce.org/examination.aspx" class="uline">ICSE</a> 2009 (from >150k students), awarded <a href="http://www.amul.com/m/amul-vidya-shree-award-std-10" class="uline">Amul Vidya Shree.</a></p>
    </li>
  </ul>
</div> -->


<div id="research">
  <h2><a name="research">Recent Publications & Technical Reports</a></h2>
  <br />
  <table width="100%" align="center" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>


      <!-- TPAMI SePiCO -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/SePiCo.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation
          </h5>
          <p>
            <a href="https://ieeexplore.ieee.org/document/10018569" class="uline-special"><span
              style="color:red">T-PAMI (IF: 24.31)</span></a>
          </p>
          <p>
            <b>Binhui Xie</b>, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang
          </p>
          <p>
            A novel one-stage adaptation framework that highlights the semantic concepts of individual pixel to promote
            learning of class-discriminative and class-balanced pixel embedding space across domains.
          </p>
          <a href="https://binhuixie.github.io/sepico-web" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Project Page</a>
          <a href="https://arxiv.org/abs/2204.08808" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/SePiCo" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
        </td>
      </tr>

      <!-- CVPR EVA -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/cvpr2023_eva.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            EVA: Exploring the Limits of Masked Visual Representation Learning at Scale
          </h5>
          <p>
            <a href="https://cvpr2023.thecvf.com" class="uline-special"><span style="color:red">CVPR 2023</span></a>
          </p>
          <p>
            Yuxin Fang, Wen Wang, <b>Binhui Xie</b>, Quan Sun, Ledell Yu Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao
          </p>
          <p>
            EVA is the first open-sourced billion-scale vision foundation model that achieves state-of-the-art performance on a broad range of downstream tasks.
          </p>
          <a href="https://arxiv.org/abs/2211.07636" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/baaivision/EVA" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
          <!-- <a href="https://www.bilibili.com/video/av692744964" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Video</a> -->
          <!-- <a href="https://www.dropbox.com/s/8ozwc8uw1q1tqlf/eada_slides.pdf?dl=0" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a> -->
          <!-- <a href="https://www.dropbox.com/s/pvb2701k2gr9cfb/aaai23poster.pdf?dl=0" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a> -->
        </td>
      </tr>

      <!-- CVPR RoTTA -->
      <!-- <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/cvpr2023_rotta.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Robust Test-Time Adaptation in Dynamic Scenarios
          </h5>
          <p>
            <a href="https://cvpr2023.thecvf.com" class="uline-special"><span style="color:red">CVPR 2023</span></a>
          </p>
          <p>
            Longhui Yuan*, <b>Binhui Xie*</b>, and Shuang Li
          </p>
          <p>
            RoTTA presents a new practical test-time adaptation setting where environments gradually change and the test data is sampled correlatively over time.
          </p>
          <a href="https://arxiv.org/abs/xxx" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/RoTTA" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
        </td>
      </tr> -->

      <!-- AAAI VBLC -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/aaai2023_VBLLC.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            VBLC: Visibility Boosting and Logit-Constraint Learning for Domain Adaptive Semantic Segmentation under
            Adverse Conditions
          </h5>
          <p>
            <a href="https://aaai.org/Conferences/AAAI-23/" class="uline-special"><span style="color:red">AAAI
                2023 (Oral)</span></a>
          </p>
          <p>
            Mingjia Li*, <b>Binhui Xie*</b>, Shuang Li, Chi Harold Liu, and Xinjing Cheng
          </p>
          <p>
            VBLC tackles the problem of domain adaptive semantic segmentation under adverse conditions, getting rid of
            reference images.
          </p>
          <a href="https://arxiv.org/abs/2211.12256" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/VBLC" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
          <a href="https://www.bilibili.com/video/av692744964" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Video</a>
            <!-- <a href="https://www.dropbox.com/s/8ozwc8uw1q1tqlf/eada_slides.pdf?dl=0" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a> -->
            <a href="https://www.dropbox.com/s/pvb2701k2gr9cfb/aaai23poster.pdf?dl=0" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
        </td>
      </tr>


      <!-- CVPR RIPU -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/cvpr2022_RIPU.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain
            Adaptive Semantic Segmentation
          </h5>
          <p>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xie_Towards_Fewer_Annotations_Active_Learning_via_Region_Impurity_and_Prediction_CVPR_2022_paper.html"
              class="uline-special"><span style="color:red">CVPR 2022 (Oral)</span></a>
          </p>
          <p>
            <b>Binhui Xie</b>, Longhui Yuan, Shuang Li, Chi Harold Liu, and Xinjing Cheng
          </p>
          <p>
            A region-based acquisition strategy for DA Segmentation queries image regions that are both diverse in
            spatial adjacency and uncertain in prediction output.
          </p>
          <a href="https://arxiv.org/abs/2111.12940" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/RIPU" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
          <a href="https://www.bilibili.com/video/BV1oS4y1e7J5?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&vd_source=2536293932098e7a347341a231b3fb8b"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Video</a>
          <a href="https://www.dropbox.com/s/a8ty5l2sut89b6l/5%20min_pre.pdf?dl=0"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a>
          <a href="https://www.dropbox.com/s/mm14k36ydirk2w8/cvpr22_poster_2x1_in-person.pdf?dl=0"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
        </td>
      </tr>

      <!-- AAAI EADA -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/aaai2022_EADA.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Active Learning for Domain Adaptation: An Energy-based Approach
          </h5>
          <p>
            <a href="https://aaai.org/Conferences/AAAI-22/" class="uline-special"><span style="color:red">AAAI
                2022</span></a>
          </p>
          <p>
            <b>Binhui Xie</b>, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang
          </p>
          <p>
            A new perspective to select a highly informative subset of unlabeled target data under domain shift via
            exploiting free energy biases across domains.
          </p>
          <a href="https://arxiv.org/abs/2112.01406" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/EADA" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
          <a href="https://www.bilibili.com/video/BV1qa411h7Xm?share_source=copy_web"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Video</a>
          <a href="https://www.dropbox.com/s/8ozwc8uw1q1tqlf/eada_slides.pdf?dl=0"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a>
          <a href="https://www.dropbox.com/s/xdtcd3iifb7xd06/eada_poster.pdf?dl=0"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
        </td>
      </tr>

      <!-- TPAMI GDCAN -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/tpami_GDCAN.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Generalized Domain Conditioned Adaptation Network
          </h5>
          <p>
            <a href="https://ieeexplore.ieee.org/abstract/document/9366353/" class="uline-special"><span
                style="color:red">T-PAMI (IF: 24.31)</span></a>
          </p>
          <p>
            Shuang Li, <b>Binhui Xie</b>, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang
          </p>
          <p>
            We develop GDCAN to automatically determine whether domain channel activations should be separately modeled
            in each attention module for domain adaptaion problem.
          </p>
          <a href="https://arxiv.org/abs/2103.12339" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/GDCAN" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
          <a href="https://www.dropbox.com/s/9h6cbyl3x8lz7bw/gdcan_poster.pdf?dl=0"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
        </td>
      </tr>

      <!-- TKDE CAF -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/tkde_CAF.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            A Collaborative Alignment Framework of Transferable Knowledge Extraction for Unsupervised Domain Adaptation
          </h5>
          <p>
            <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69" class="uline-special"><span
                style="color:red">TKDE</span></a>
          </p>
          <p>
            <b>Binhui Xie</b>, Shuang Li, Fangrui Lv, Chi Harold Liu, Guoren Wang, and Dapeng Wu
          </p>
          <p>
            We introduce a collaborative alignment framework that integrates global structure information and local
            semantic consistency into a unified deep model.
          </p>
          <a href="https://ieeexplore.ieee.org/document/9803869" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/CAF" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
        </td>
      </tr>


      <!-- MM SSAN -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/mm2020_SSAN.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Simultaneous Semantic Alignment Network for Heterogeneous Domain Adaptation
          </h5>
          <p>
            <a href="https://2020.acmmm.org" class="uline-special"><span style="color:red">MM 2020</span></a>
          </p>
          <p>
            Shuang Li, <b>Binhui Xie</b>, Jiashu Wu, Ying Zhao, Chi Harold Liu, and Zhengming Ding
          </p>
          <p>
            A HDA method simultaneously exploits correlations among categories and aligns the centroids for each
            category across domains.
          </p>
          <a href="https://arxiv.org/abs/2008.01677" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/SSAN" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
          <a href="https://www.dropbox.com/s/k7lu7zvkfzlv2nj/ssan_slides.pdf?dl=0"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a>
          <a href="https://www.dropbox.com/s/jjy5itt5niqp8b0/ssan_poster.pdf?dl=0"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
        </td>
      </tr>


      <!-- AAAI BCDM -->
      <!-- <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/aaai2021_BCDM.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                      Bi-classifier determinacy maximization for unsupervised domain adaptation
                  </h5>
                  <p>
                      <a href="https://aaai.org/Conferences/AAAI-22/" class="uline-special"><span style="color:red">AAAI 2021</span></a>
                  </p>
                  <p>
                      Shuang Li, Fangrui Lv, <b>Binhui Xie</b>, Chi Harold Liu, Jian Liang, Chen Qin
                  </p>
                  <p>
                      We design a novel CDD metric, which formulates classifier discrepancy as the class relevance of distinct target predictions and implicitly introduces constraint on the target feature discriminability.
                  </p>
                  <a href="https://arxiv.org/abs/2012.06995" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/BIT-DA/BCDM" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
              </td>
          </tr> -->


      <!-- MM JADA -->
      <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/mm2019_JADA.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                      Joint Adversarial Domain Adaptation
                  </h5>
                  <p>
                      <a href="https://2019.acmmm.org" class="uline-special"><span style="color:red">MM 2019</span></a>
                  </p>
                  <p>
                      Shuang Li, Chi Harold Liu, <b>Binhui Xie</b>, Limin Su, Zhengming Ding, and Gao Huang
                  </p>
                  <p>
                      JADA simultaneously aligns domain-wise and class-wise distributions across source and target in a unified adversarial learning process.
                  </p>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3343031.3351070" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/BIT-DA/JADA" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
                  <a href="https://www.dropbox.com/s/t6kxl5se1t7rla7/jada_poster.pdf?dl=0" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
              </td>
          </tr>


      <!-- TCYB CSDN -->
      <!-- <tr>
            <td valign="middle" width="35%">
                <div class="one" style="text-align:center;">
                    <img src="images/tcyb_CSDN.png" style="max-height: 300px;">
                </div>
            </td>
            <td valign="middle" width="65%">
                <h5>
                  Critical Classes and Samples Discovering for Partial Domain Adaptation
                </h5>
                <p>
                    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036" class="uline-special"><span style="color:red">T-CYB</span></a>
                </p>
                <p>
                  Shuang Li, Kaixiong Gong, <b>Binhui Xie</b>, Chi Harold Liu, Weipeng Cao, Song Tian
                </p>
                <p>
                    CSDN, which aims to solve PDA problem, identifies the most relevant source classes and critical target samples, such that more precise cross-domain alignment in the shared label space could be enforced by co-training two diverse classifiers. 
                </p>
                <a href="https://ieeexplore.ieee.org/document/9756672" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                <a href="https://github.com/BIT-DA/CSDN" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
            </td>
        </tr> -->


      <!-- AAAI DCAN -->
      <!-- <tr>
            <td valign="middle" width="35%">
                <div class="one" style="text-align:center;">
                    <div class="two" id="jump_image" style="opacity: 0;"></div> 
                    <img src="images/aaai2020_DCAN.png" style="max-height: 300px;">
                </div>
            </td>
            <td valign="middle" width="65%">
                <h5>
                    Domain Conditioned Adaptation Network
                </h5>
                <p>
                    <a href="https://aaai.org/Conferences/AAAI-22/" class="uline-special"><span style="color:red">AAAI 2020</span></a>
                </p>
                <p>
                    Shuang Li, Chi Harold Liu, Qiuxia Lin, <b>Binhui Xie</b>, Zhengming Ding, Gao Huang, Jian Tang
                </p>
                <p>
                    DCAN relaxes the shared-convnets assumption and excites distinct convolutional channels with a domain conditioned channel attention mechanism.
                </p>
                <a href="https://arxiv.org/abs/2005.06717" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                <a href="https://github.com/BIT-DA/DCAN" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
            </td>
        </tr> -->


      <!-- TNNLS DTLC -->
      <!-- <tr>
          <td valign="middle" width="35%">
              <div class="one" style="text-align:center;">
                  <img src="images/tnnls_DTLC.png" style="max-height: 300px;">
              </div>
          </td>
          <td valign="middle" width="65%">
              <h5>
                Discriminative transfer feature and label consistency for cross-domain image classification
              </h5>
              <p>
                  <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" class="uline-special"><span style="color:red">T-NNLS</span></a>
              </p>
              <p>
                Shuang Li, Chi Harold Liu, Limin Su, <b>Binhui Xie</b>, Zhengming Ding, CL Philip Chen, Dapeng Wu
              </p>
              <p>
                DTLC can naturally unify cross-domain alignment with discriminative information preserved and label consistency of source and target data into one framework.
              </p>
              <a href="https://ieeexplore.ieee.org/abstract/document/8951259" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
              <a href="https://github.com/hnjzlishuang/DTLC" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
          </td>
      </tr> -->
  
    </tbody>
  </table>
</div>

<hr>

<!-- <div id="projects">
  <h2>Projects</h2>
  <table width="100%" align="center" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
      <tr>
        <td valign="top" width="50%">
          <h5>
            Fabrik: An Online Collaborative Neural Network Editor
          </h5>

          <p class="authors">
            Utsav Garg, <b>Viraj Prabhu</b>, Deshraj Yadav, Ram Ramrakhya, Harsh Agrawal, Dhruv Batra
          </p>
          <p class="authors">Lead mentor on Fabrik, an open-source web platform to collaboratively build, visualize, and
            design neural networks in the browser.</p>
          <a href="https://arxiv.org/abs/1810.11649" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Report</a>
          <a href="https://github.com/Cloud-CV/Fabrik" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>

        </td>
        <td width="50%">
          <div class="one" style="text-align:center;">
            <div class="two" id="jump_image" style="opacity: 0;"></div>
            <img src="images/fabrik.png" style="max-height: 150px;width:100%">
          </div>
        </td>
      </tr>

      <tr>
        <td valign="top" width="40%">
          <h5>PyTorch implementation of <a href="https://arxiv.org/abs/1703.06585" class="uline">Learning Cooperative
              Visual Dialog Agents with Deep Reinforcement Learning</a></h5>
          <p class="authors">
            Nirbhay Modhe, <b>Viraj Prabhu</b>, Michael Cogswell, Satwik Kottur, Abhishek Das, Stefan Lee, Devi Parikh,
            Dhruv Batra
          </p>
          <a href="https://github.com/batra-mlp-lab/visdial-rl" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>

        </td>
        <td width="50%">
          <div class="one" style="text-align:center;">
            <div class="two" id="jump_image" style="opacity: 0;"></div>
            <img src="images/visdial_rl.jpg" style="margin-top:30px;max-height: 200px;width:100%;">
          </div>
        </td>
      </tr>
      <td valign="top" width="75%">
        <h5>Adobe Captivate Prime</h5>
        <p class="authors">During my time as a software developer at Adobe (Aug '15-'16), I was responsible for the <a
            href="http://www.adobe.com/products/captivateprime.html" class="uline">Captivate Prime</a> Android app
          through two release cycles. I developed features for offline content play-back, syncing, and UI.
        </p>
      </td>
      <td width="25%">
        <div class="one" style="text-align:center;">
          <div class="two" style="opacity: 0;"></div>
          <img src="images/cp.jpeg" style="width:50%;">
        </div>
      </td>
      </tr>
      <tr>
        <td valign="top" width="75%">
          <h5>Automated camera calibration and boresighting</h5>
          <p class="authors">Over a research internship at <a href="http://tonboimaging.com/" class="uline">Tonbo
              Imaging</a> (Spring '15), I designed an algorithm for automated camera calibration and implemented a
            boresighting algorithm for the company's video precision boresight tool.
          </p>
          </p>
        </td>
        <td width="25%">
          <div class="one" style="text-align:center;">
            <div class="two" style="opacity: 0;"></div>
            <img src="images/tonbo.png" style="width:50%;">
          </div>
        </td>
      </tr>
      <tr>
        <td valign="top" width="75%">
          <h5>KeyframeCut</h5>
          <p class="authors">Over an internship at Adobe, I co-developed KeyframeCut, a fast graphcut-based segmentation
            algorithm for real-time background substitution in video. Transferred into Magic Green Screen, the marquee
            feature of Adobe Presenter Video Express 11.</p>
          <a href="https://www.youtube.com/watch?v=wM9mpFxaOpM" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Demo</a>
          <a href="http://blogs.adobe.com/captivate/2016/03/adobe-presenter-video-express-the-end-of-green-screen.html"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Blog</a>
          </p>
        </td>
        <td width="25%">
          <div class="one" style="text-align:center;">
            <div class="two" style="opacity: 0;"></div>
            <img src="images/mgs.png" style="width:75%;">
          </div>
        </td>
      </tr>
    </tbody>
  </table>
</div> -->

<div id="talks">
  <h2>Invited Talks</h2>
  <br />
  <ul>
    <li>
      <p>
        2022.6, Synced (<a href="https://www.bilibili.com/video/BV19a411p7tg/?spm_id_from=333.999.0.0"
          class="uline">online</a>), Towards Fewer Annotations: Active Learning for Adaptive Semantic Segmentation.
      </p>
    </li>
    <li>
      <p>
        2022.6, ReadPaper (<a
          href="https://www.bilibili.com/video/BV1oS4y1e7J5/?spm_id_from=333.999.0.0&vd_source=2536293932098e7a347341a231b3fb8b"
          class="uline">online</a>), Towards Fewer Annotations: Active Learning for Adaptive Semantic Segmentation.
      </p>
    </li>
    <li>
      <p>
        2022.5, Zhidx Course (<a href="https://course.zhidx.com/c/Njg5MGIyMDlkYjhkZmVhMmI5ZTM="
          class="uline">onine</a>), Active Learning under Distribution Shift.
      </p>
    </li>
    <li>
      <p>
        2022.3, AI Drive (<a
          href="https://www.bilibili.com/video/BV1qa411h7Xm/?spm_id_from=333.999.0.0&vd_source=2536293932098e7a347341a231b3fb8b"
          class="uline">online</a>), Energy-based Active Domain Adaptation.
      </p>
    </li>
    <li>
      <p>
        2021.10, VALSE (Hangzhou), Generalized Domain Conditioned Adaptation Network.
      </p>
    </li>
  </ul>

</div>

<hr>

<div class="news">
  <h2>Academic Service</h2>
  <br />
  <p>
    Organizer of Workshop: VALSE 2022 (Student Workshop).
  </p>
  <p>
    Reviewer for ICCV'23, CVPR'23, ICLR'23, AAAI'23, ECCV'22, CVPR'22, AAAI'22, ICCV'21, CVPR'21, AAAI'21, etc.
  </p>
  <p>
    Reviewer for TPAMI, TNNLS, TMM, etc.
  </p>
</div>


<hr>
<div class="news">
  <h2>Teaching Assistant</h2>
  <br />
  <p>
    Introduction to Machine Learning, since 2021, Instructor: <a href="https://shuangli.xyz" class="uline">Prof. Li</a>
  </p>

</div>


<hr>
<div id="news">
  <h2>Selected Honors and Awards</h2>
  <ul>
    <!-- <li>
      <p>
        CETC GUORUI Scholarship, 2021.
      </p>
    </li> -->
    <li>
      <p>
        Excellent Undergraduate in Beijing Institute of Technology, 2019.
      </p>
    </li>
    <!-- <li>
      <p>
        Second Class Scholarship, Beijing Institute of Technology, 2018.
      </p>
    </li> -->
  </ul>
</div>

<hr>
<div class="news">
  <h2>Contact</h2>
  <br />
  <ul>
    <li>
      <p>
        Email: binhuixie@bit.edu.cn
      </p>
    <li>
      <p>
        Address: Room 1106, Central Teaching Building, Beijing Institute of Technology, Beijing
      </p>
    </li>
  </ul>

</div>

<hr>