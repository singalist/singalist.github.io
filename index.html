---
layout: default
tags: about
---
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script type="text/javascript">
  function readMore() {
    $('#readMore').hide();
    $('#more').show();
  }
  function readLess() {
    $('#readMore').show();
    $('#more').hide();
  }
</script>
<img src="{{ site.baseurl }}/images/photo.jpg" alt="Wenxuan Ma" width="180"
  style="float: right; padding: 5px 0 0 15px; border-radius: 0%;" />

<div class="bio" style="text-align:justify">
  <!-- <hr/> -->
  <p>
    I am a third-year master's student at <a href="https://www.bit.edu.cn" class="uline">Beijing Institute of Technology</a>, advised by <a href="https://shuangli.xyz" class="uline">Prof. Shuang Li</a>. I received my Bachelor's degree in Automation from Beijing Institute of Technology (2017-2021).
  </p>

  <p>
    My research interest includes learning improved representation for transfer learning, domain adaptation, and data-efficient learning in computer vision tasks. I am currently working on knowledge transfer across modalities. 
  </p>

  <p>
    <b>Contact</b>: I'm always happy to discuss or collaborate! Feel free to drop me an <a
      href="mailto:wenxuanma@bit.edu.cn" class="uline">email</a> if you're interested.
  </p>

  <br />

  <!-- <div class="container">
    <div class="row" style="text-align:center;">
      <div class="col-4">
        <div class="row">
          <div class="col-6">
            <a href="http://www.bits-pilani.ac.in/"><img src="images/bits.png" style="max-height:150px;width:80%"></a>
          </div>
          <div class="col-6">
            <a href="https://www.gatech.edu/"><img src="images/gt.jpg" style="max-height:150px;width:80%"></a>
          </div>
        </div>
      </div>
      <div class="col-8">
        <div class="row" style="text-align:center;">
          <div class="col-3">
            <a href="https://www.adobe.com/in/"><img src="images/adobe.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="http://curai.com/"><img src="images/curai.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="https://einstein.ai/"><img src="images/sf.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="https://nv-tlabs.github.io/"><img src="images/nvidia.png" style="width:80%;max-height:150px"></a>
          </div>
        </div>
      </div>
    </div>
    <div class="row" style="text-align:center;">
      <div class="col-4">
        <div class="row">
          <div class="col-6">
            <div style="padding:10px">
              <h6>Fall '11-Spring '15</h6>
            </div>
          </div>
          <div class="col-6">
            <div style="padding:10px">
              <h6>Fall '17-current</h6>
            </div>
          </div>
        </div>
      </div>
      <div class="col-8">
        <div class="row" style="text-align:center;">
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '14, Fall '15- Summer '16</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '18, '19</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '21</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '22</h6>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div> -->
  
</div>


<hr />
<div class="news">
  <h2>News</h2>
  <br />
  <ul>

    <li>
      <p>[Sep 2023] Our <b>LSG</b> for data-efficient image/video/audio learning is accepted by <font color="red"><b>NeurIPS</b></font> 2023!</p>
    </li>

    <li>
      <p>[July 2023] Our <b>BorLan</b> for data-efficient visual learning is accepted by <font color="red"><b>ICCV</b></font> 2023!</p>
    </li>

    <!-- <li>
      <p>[July 2022] Our DOT for ViT-based unsupervised domain adaptation is accepted by <font color="red">ACM MM</font> 2022!</p>
    </li>

    <li>
      <p>[Sep 2021] Our MetaReg for unsupervised domain adaptation is accepted by <font color="red">TKDE</font> (IF: 8.9)!</p>
  </li>

    <li>
      <p>[Feb 2021] Our DDA for resource-efficient unsupervised domain adaptation is accepted by <font color="red">CVPR</font> 2021!</p>
    </li> -->

    <!-- <li>
      <p>[Feb 2023] Two papers on representation learning (<a href="https://arxiv.org/abs/2211.07636" class="uline">EVA <font color="red">highlight</font></a>) and test-time adaptation (<a href="https://arxiv.org/abs/2303.13899" class="uline">RoTTA</a>) are accepted by CVPR 2023!</p>
    </li> -->

  <!-- </ul>
  <a id="readMore" class="uline" href="#" onclick="readMore();return false;">
    <div style='text-align: center;'>
      <h4>Read More</h4>
    </div>
  </a>
  <span id="more">
    <ul>

      <li>
        <p> [Jan 2020] Our <a href="https://ieeexplore.ieee.org/document/8951259" class="uline">DTLC</a> for shallow domain adaptation is accepted by TNNLS! </p>
    </li>

      <li>
        <p>[Dec 2019] Our <a href='https://arxiv.org/abs/2005.06717' class='uline'>DCAN</a> for domain adaptation is accepted by AAAI 2020!</p>
      </li>

      <li>
        <p>[Jul 2019] Our <a href='https://dl.acm.org/doi/abs/10.1145/3343031.3351070' class='uline'>JADA</a> (my starting research journey) is accepted by ACM MM 2019!</p>
      </li>
  </span>
  </ul>
  <a id="readLess" class="uline" href="#" onclick="readLess();return false;">
    <div style='text-align: center;'>
      <h4>Read Less</h4>
    </div>
  </a> -->
</div>

<hr />

<!-- <hr>
<div class="Achievements">
  <h1>Achievements</h1>
  <ul>
    <li>
      <p>Among the top 30 percent highest scoring reviewers for NeurIPS 2018.</p>
    </li>
    <li>
      <p>Winner of <a href="https://vthacks-iv.devpost.com/submissions" class="uline">VT Hacks 2017</a>, a <a href="https://mlh.io/" class="uline">Major League Hacking</a> event.</p>
    </li>
    <li>
      <p>Awarded travel scholarship to represent CloudCV at Google Summer of Code Mentor Summit 2016, 2017.</p>
    </li>
    <li>
      <p>Winner, Google Hackathon, <a href="https://bits-apogee.org/" class="uline">APOGEE</a> 2014 (BITS Pilani's annual technical symposium).</p>
    </li>
    <li>
      <p>Second Place, Project Presentation (Adaptive Technology and Design Appliances Tracks), <a href="https://bits-apogee.org/" class="uline">APOGEE</a> 2013.</p>
    </li>
    <li>
    <p>Top 0.2%, <a href="http://bitsadmission.com/bitsatmain.aspx" class="uline">BITSAT</a> 2011 (from >120k students).</p>
    </li>
    <li>
      <p>Top 0.1%, <a href="http://www.cisce.org/examination.aspx" class="uline">ICSE</a> 2009 (from >150k students), awarded <a href="http://www.amul.com/m/amul-vidya-shree-award-std-10" class="uline">Amul Vidya Shree.</a></p>
    </li>
  </ul>
</div> -->


<div id="research">
  <h2><a name="research">Recent Publications</a></h2>
  <br />
  <table width="100%" align="center" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>

       <!-- NeurIPS Language Semantic Graph -->
       <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/nips23_lsg.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Language Semantic Graph Guided Data-Efficient Learning
          </h5>
          <p>
            <a href="https://nips.cc/" class="uline-special"><span
              style="color:red">NeurIPS 2023</span></a>
          </p>
          <p>
            <b>Wenxuan Ma</b>, Shuang Li*, Lincan Cai, Jingxuan Kang
          </p>
          <p>
            We leverage graph neural network to model language semantic space and benefit data-efficient training.
          </p>
          <!-- <a href="https://singalist.github.io/" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Project Page (coming)</a> -->
          <a href="https://arxiv.org/abs/2311.08782" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/LSG" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
        </td>
      </tr>

      <!-- ICCV BorLan -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/iccv23_borlan.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Borrowing Knowledge From Pre-trained Language Model: A New Data-Efficient Visual Learning Paradigm
          </h5>
          <p>
            <a href="https://iccv2023.thecvf.com/" class="uline-special"><span
              style="color:red">ICCV 2023</span></a>
          </p>
          <p>
            <b>Wenxuan Ma</b>, Shuang Li*, Jinming Zhang, Chi Harold Liu, Jingxuan Kang, Yulin Wang, Gao Huang
          </p>
          <p>
            We leverage pre-trained language model to capture semantic relations between visual categories and promote data-efficient training on vision models.
          </p>
          <!-- <a href="https://singalist.github.io/" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Project Page (coming)</a> -->
          <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Borrowing_Knowledge_From_Pre-trained_Language_Model_A_New_Data-efficient_Visual_ICCV_2023_paper.html" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/BorLan" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
          <a href="https://www.bilibili.com/video/BV14p4y1V7Ym/?spm_id_from=333.788.recommend_more_video.-1&vd_source=9449892be0c9e3f0d9ef47074a426848" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Talk</a>
        </td>
      </tr>

      <!-- TNNLS SQAdapt -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/tnnls23_sqadapt.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Source-Free Active Domain Adaptation via Augmentation-Based Sample Query and Progressive Model Adaptation
          </h5>
          <p>
            <a><span
              style="color:red">TNNLS 2023</span></a>
          </p>
          <p>
            Shuang Li, Rui Zhang, Kaixiong Gong, Mixue Xie, <b>Wenxuan Ma</b>, Guangyu Gao
          </p>
          <p>
            We investigate the novel problem of source-free active domain adaptation, which considers the inaccessibility of the source domain data during active domain adaptation. 
          </p>
          <!-- <a href="https://singalist.github.io/" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Project Page (coming)</a> -->
          <a href="https://ieeexplore.ieee.org/abstract/document/10368185" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <!-- <a href="https://github.com/BIT-DA/Domain-Oriented-Transformer" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a> -->
        </td>
      </tr>

      <!-- ACM MM DOT -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/mm22_dot.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Making the Best of Both Worlds: A Domain-Oriented Transformer for Unsupervised Domain Adaptation
          </h5>
          <p>
            <a href="https://2022.acmmm.org/" class="uline-special"><span
              style="color:red">ACM MM 2022</span></a>
          </p>
          <p>
            <b>Wenxuan Ma</b>, Jinming Zhang, Shuang Li*, Chi Harold Liu, Yulin Wang, Wei Li
          </p>
          <p>
            We propose to learn two embedding spaces simultaneously, one source domain-oriented and the other target domain-oriented in unsupervised domain adaptation. This is achieved using two domain-specific tokens in vision transformer.
          </p>
          <!-- <a href="https://singalist.github.io/" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Project Page (coming)</a> -->
          <a href="https://arxiv.org/abs/2208.01195" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/Domain-Oriented-Transformer" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
        </td>
      </tr>

      <!-- TKDE MetaReg -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/tkde21_metareg.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Meta-reweighted Regularization for Unsupervised Domain Adaptation
          </h5>
          <p>
            <a><span
              style="color:red">TKDE 2021</span></a>
          </p>
          <p>
            Shuang Li, <b>Wenxuan Ma</b>, Jinming Zhang, Chi Harold Liu*, Jian Liang
          </p>
          <p>
            MetaReg is a self-training based domain adaptation method. We automatically assign optimal weight for target data to reduce the noise of target pseudo-labels. The optimal weights are obtained via meta-learning.
          </p>
          <!-- <a href="https://singalist.github.io/" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Project Page (coming)</a> -->
          <a href="https://ieeexplore.ieee.org/document/9546671" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/MetaReg" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
        </td>
      </tr>

      <!-- CVPR DDA -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/cvpr21_dda.png" style="max-height: 300;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            Dynamic Domain Adaptation for Efficient Inference
          </h5>
          <p>
            <a href="https://cvpr2021.thecvf.com/" class="uline-special"><span
              style="color:red">CVPR 2021</span></a>
          </p>
          <p>
            Shuang Li, Jinming Zhang, <b>Wenxuan Ma</b>, Chi Harold Liu, Wei Li
          </p>
          <p>
            We explore new settings in domain adaptation that considers the computation resource constrains during inference.
          </p>
          <!-- <a href="https://singalist.github.io/" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Project Page (coming)</a> -->
          <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Dynamic_Domain_Adaptation_for_Efficient_Inference_CVPR_2021_paper.html" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/BIT-DA/DDA" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
          <a href="https://www.bilibili.com/video/BV1R5411P7bE?p=3&vd_source=9449892be0c9e3f0d9ef47074a426848" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Talk</a>
        </td>
      </tr>

    </tbody>
  </table>
</div>

<hr>

<div id="talks">
  <h2>Invited Talks</h2>
  <br />
  <ul>
    <li>
      <p>
        2021.3, Qingyuan Live (<a href="https://www.bilibili.com/video/BV1R5411P7bE?p=3&vd_source=9449892be0c9e3f0d9ef47074a426848"
          class="uline">online</a>), Dynamic Domain Adaptation for Efficient Inference.
      </p>
    </li>
    <!-- <li>
      <p>
        2022.6, ReadPaper (<a
          href="https://www.bilibili.com/video/BV1oS4y1e7J5/?spm_id_from=333.999.0.0&vd_source=2536293932098e7a347341a231b3fb8b"
          class="uline">online</a>), Towards Fewer Annotations: Active Learning for Adaptive Semantic Segmentation.
      </p>
    </li>
    <li>
      <p>
        2022.5, Zhidx Course (<a href="https://course.zhidx.com/c/Njg5MGIyMDlkYjhkZmVhMmI5ZTM="
          class="uline">onine</a>), Active Learning under Distribution Shift.
      </p>
    </li> -->
    <li>
      <p>
        2022.3, AI Time (<a
          href="https://www.bilibili.com/video/BV14p4y1V7Ym/?spm_id_from=333.788.recommend_more_video.-1&vd_source=9449892be0c9e3f0d9ef47074a426848"
          class="uline">online</a>), Borrowing Knowledge From Pre-trained Language Model: A New Data-efficient Visual Learning Paradigm (BorLan).
      </p>
    </li>
    <!--<li>
      <p>
        2021.10, VALSE (Hangzhou), Generalized Domain Conditioned Adaptation Network.
      </p>
    </li> -->
  </ul>

</div>

<hr>

<div class="news">
  <h2>Academic Service</h2>
  <br />
  <!-- <p>
    Organizer of Workshop: VALSE 2022 (Student Workshop).
  </p> -->
  <!-- <p>
    Conference Reviewer for ICLR'24, AAAI'24, NeurIPS'23, ICCV'23, CVPR'23, ICLR'23, AAAI'23, ECCV'22, CVPR'22, AAAI'22, ICCV'21, CVPR'21, AAAI'21, etc.
  </p> -->
  <p>
    Journal Reviewer for TPAMI, TNNLS, TETC, TIICM, etc.
  </p>
  <p>
    Conference Reviewer for CVPR.
  </p>
</div>


<hr>
<div class="news">
  <h2>Teaching Assistant</h2>
  <br />
  <p>
    Machine Learning Essentials, Spring 2023, Instructor: <a href="https://shuangli.xyz" class="uline">Prof. Li</a>
  </p>

</div>


<!-- <hr>
<div id="news">
  <h2>Honors and Awards</h2>
  <ul>
    <li>
      <p>
        3rd place, VisDA-2022@NeurIPS, 2022.
      </p>
    <li>
    <li>
      <p>
        Special Academic Scholarship, Beijing Institute of Technology, 2022.
      </p>
    <li>
      <p>
        CETC GUORUI Scholarship, 2021.
      </p>
    </li>
    <li>
      <p>
        Excellent Undergraduate & Graduation Thesis at Beijing Institute of Technology, 2019.
      </p>
    </li>
  </ul>
</div> -->

<hr>
<div class="news">
  <h2>Contact</h2>
  <br />
  <ul>
    <li>
      <p>
        Email: wenxuanma@bit.edu.cn
      </p>
    <li>
      <p>
        Address: Room 1103, Central Teaching Building, Beijing Institute of Technology, Beijing
      </p>
    </li>
  </ul>

</div>

<hr>